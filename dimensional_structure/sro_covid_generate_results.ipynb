{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook reports analyses for the following paper: \n",
    "## Relating psychiatric symptoms and self-regulation during the COVID-19 crisis\n",
    "\n",
    "This notebook:\n",
    "\n",
    "* Import training datasets for each set of variables:'task', 'survey', 'psych', 'crisis', 'health risk outcomes'\n",
    "* Remove variables from training datasets that are not needed \n",
    "* EFA on tasks (N = 386, vars = 113), surveys (N = 386, vars = 61), health risk outcomes (N = 386, vars = 50), psych (N = 497, vars = 207, number of factor is specified based on previous solution from Rouault  et al.,2018 ), crisis (N = 2868, vars = 37)\n",
    "* Comparison between solution obtained by Eisenberg et al., 2019 and the one here on tasks and surveys\n",
    "* Evaluate solution obtained (Fit indexes, Variance accounted for, Plot factor correlation, Plot BIC) \n",
    "* Cross prediction for tasks and surveys\n",
    "* Get testing datasets \n",
    "* Predict factor scores from solution obtained \n",
    "* Remove subjects reporting uncompatible demographics between the first and the second assessment\n",
    "* Put together file for stats analyses\n",
    "* Run prediction analysis and plot results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries and functions** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from dimensional_structure.results_mv import Results\n",
    "from selfregulation.utils.utils import (get_info, filter_behav_data,get_behav_data, \n",
    "                                        get_admin_data,get_demographics, drop_constant_column, \n",
    "                                        remove_variables_from_df, drop_not_common_vars, remove_suffix_row, \n",
    "                                        get_predicted_score_data)\n",
    "\n",
    "from selfregulation.utils.data_preparation_utils import check_age, impute_age, check_demo_vars\n",
    "from selfregulation.utils.result_utils import (get_loadings, load_results, get_fit_indexes, get_EFA, get_HCA,\n",
    "                                               get_short_names, \n",
    "                                               get_var_of_interest, get_paired_diff)\n",
    "                                               \n",
    "from selfregulation.utils.plot_utils import save_figure, plot_correlation\n",
    "\n",
    "from shutil import copyfile\n",
    "from os import makedirs, path, remove\n",
    "import json\n",
    "import pkg_resources\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import argparse\n",
    "import pkg_resources\n",
    "import random\n",
    "import time\n",
    "from cross_results_plots import get_EFA_HCA, plot_corr_heatmap\n",
    "from dimensional_structure.cross_results_utils import run_cross_prediction\n",
    "from dimensional_structure.cross_results_plots import plot_cross_within_prediction, plot_BIC\n",
    "from dimensional_structure.utils import get_factor_groups, residualize_baseline\n",
    "from dimensional_structure.EFA_plots import (plot_factor_correlation, rename_factor_names,format_short_loadings_names,\n",
    "                                             plot_short_factor_loading)\n",
    "\n",
    "from dimensional_structure.prediction_plots import (get_predictions, plot_prediction, visualize_importance, \n",
    "                                                    plot_fingerprint)\n",
    "\n",
    "\n",
    "from dimensional_structure.prediction_utils import run_prediction, run_prediction_local\n",
    "\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects\n",
    "\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for EFA on training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_analysis = True\n",
    "bootstrap = False\n",
    "boot_iter = 1000\n",
    "selected_subsets = ['task', 'survey', 'psych', 'crisis']\n",
    "verbose = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir=get_info('base_directory')\n",
    "config_file = path.join(basedir, 'dimensional_structure', 'sro_covid_config.json')\n",
    "subsets = json.load(open(config_file,'r'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "IMPORTING TRAINING DATASET: task\n",
      "********************************************************************************\n",
      "Getting dataset: /SRO/Data_master/Complete_02-16-2019...:\n",
      "file: meaningful_variables_pt_imputed_train.csv \n",
      " \n",
      "Dropping ravens.score.pt as included as explanatory variable in stats model\n",
      "Dimensions training dataset (386, 113)\n",
      "********************************************************************************\n",
      "IMPORTING TRAINING DATASET: survey\n",
      "********************************************************************************\n",
      "Getting dataset: /SRO/Data_master/Complete_02-16-2019...:\n",
      "file: meaningful_variables_pt_imputed_train.csv \n",
      " \n",
      "Dropping ['bis11_survey.Attentional.pt', 'bis11_survey.Motor.pt', 'bis11_survey.Nonplanning.pt'] as BIS included for Psych\n",
      "Dimensions training dataset (386, 61)\n",
      "********************************************************************************\n",
      "IMPORTING TRAINING DATASET: psych\n",
      "********************************************************************************\n",
      "Getting dataset: /SRO/Data/Complete_Covid_03-26-2021...:\n",
      "file: subject_x_items_psych_sro.csv \n",
      " \n",
      "Getting dataset: /SRO/Data/Complete_Covid_03-26-2021...:\n",
      "file: subject_x_items_psych_rou.csv \n",
      " \n",
      "Dropping {'eat.9', 'eat.26'} because they had no variability in testing dataset\n",
      "Dimensions training dataset (497, 207)\n",
      "********************************************************************************\n",
      "IMPORTING TRAINING DATASET: crisis\n",
      "********************************************************************************\n",
      "Getting dataset: /SRO/Data_crisis/PA_Data_Sharing/April/Data...:\n",
      "file: CRISIS_Adult_April_2020_training_covid_imputed.csv \n",
      " \n",
      "Dropping variables referring to period before COVID-19\n",
      "Dimensions training dataset (2868, 37)\n",
      "********************************************************************************\n",
      "IMPORTING TRAINING DATASET: health and risk behavior\n",
      "********************************************************************************\n",
      "Getting dataset: /SRO/Data_master/Complete_02-16-2019...:\n",
      "file: demographic_health.csv \n",
      " \n",
      "Dropping ['BlackoutFlashbackDrugUse', 'NeglectedFamilyDrugUse', 'MedicalProblemsDueToDrugUse', 'leisure_time_activity_survey.activity_level'] because either no variability or not included as health risk behavior\n",
      "Dimensions training dataset (386, 50)\n"
     ]
    }
   ],
   "source": [
    "data={}\n",
    "for subset in subsets:\n",
    "    name = subset['name']\n",
    "    if selected_subsets is not None and name not in selected_subsets:\n",
    "        continue\n",
    "    print('*'*80)\n",
    "    print('IMPORTING TRAINING DATASET:', name)\n",
    "    print('*'*80)\n",
    "    if name =='psych':\n",
    "                \n",
    "        psych_sro = get_behav_data(file = 'subject_x_items_psych_sro.csv', verbose = True)\n",
    "        psych_rou = get_behav_data(file = 'subject_x_items_psych_rou.csv', verbose = True)\n",
    "\n",
    "        psych_sro, sro_colname_drop = drop_constant_column(psych_sro, 'SRO') \n",
    "        psych_rou, rou_colname_drop = drop_constant_column(psych_rou, 'ROU') \n",
    "\n",
    "        print('Dropping', sro_colname_drop, 'because they had no variability in testing dataset')\n",
    "        psych_rou = remove_variables_from_df(psych_rou, sro_colname_drop) \n",
    "        data[name] = psych_rou\n",
    "        \n",
    "        print('Dimensions training dataset', data[name].shape)\n",
    "        \n",
    "    elif name == 'crisis': \n",
    "        df = get_behav_data(file = 'CRISIS_Adult_April_2020_training_covid_imputed.csv', \n",
    "                                    data_subset = 'Data_crisis',  verbose=True)\n",
    "        \n",
    "        print('Dropping variables referring to period before COVID-19')\n",
    "        df = df[df.columns.drop(list(df.filter(regex='pre')))] # remove all vars with pre \n",
    "        data[name] = df\n",
    "        print('Dimensions training dataset', data[name].shape)\n",
    "    else: \n",
    "        \n",
    "        data[name] = get_behav_data(file = 'meaningful_variables_pt_imputed_train.csv', \n",
    "                                    data_subset = 'Data_master', filter_regex=name, verbose=True)\n",
    "        if name == 'task':\n",
    "            list_vars_task = 'ravens.score.pt'\n",
    "            \n",
    "            print('Dropping', list_vars_task, 'as included as explanatory variable in stats model')\n",
    "            data[name] = data[name].drop(list_vars_task, axis=1)\n",
    "            print('Dimensions training dataset', data[name].shape)\n",
    "            \n",
    "        if name =='survey': \n",
    "            list_vars_survey = ['bis11_survey.Attentional.pt', 'bis11_survey.Motor.pt', 'bis11_survey.Nonplanning.pt']            \n",
    "\n",
    "            print('Dropping', list_vars_survey ,'as BIS included for Psych')\n",
    "            data[name]= data[name].drop(list_vars_survey, axis =1)\n",
    "            print('Dimensions training dataset', data[name].shape)\n",
    "            \n",
    "print('*'*80)\n",
    "print('IMPORTING TRAINING DATASET:', 'health and risk behavior')\n",
    "print('*'*80)\n",
    "demo_data = get_demographics(data_subset = 'Data_master')\n",
    "demo_data_training = demo_data.loc[data['task'].index]\n",
    "\n",
    "list_vars =['BlackoutFlashbackDrugUse', 'NeglectedFamilyDrugUse', 'MedicalProblemsDueToDrugUse', \n",
    "            'leisure_time_activity_survey.activity_level']\n",
    "print('Dropping', list_vars ,'because either no variability or not included as health risk behavior')\n",
    "demo_data_training= demo_data_training.drop(list_vars, axis =1)\n",
    "\n",
    "print('Dimensions training dataset', demo_data_training.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run EFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "SUBSET: TASK\n",
      "*******************************************************************************\n",
      "*******************************************************************************\n",
      "Analyzing Subset: task\n",
      "Dataset imputed: number of variables, 113 , number of subjects, 386\n",
      "*******************************************************************************\n",
      "Running demographics\n",
      "*******************************************************************************\n",
      "Is the data adequate for factor analysis? Yes\n",
      "Determining Optimal Dimensionality\n",
      "Best Components:  {'c_metric-BIC': 6}\n",
      "Creating Factor Tree\n",
      "No 6 factor solution computed yet! Computing...\n",
      "Determining Higher Order Factors\n",
      "# of components not specified, using BIC determined #\n",
      "*******************************************************************************\n",
      "Running EFA, rotate: oblimin\n",
      "*******************************************************************************\n",
      "Is the data adequate for factor analysis? Yes\n",
      "Determining Optimal Dimensionality\n",
      "Best Components:  {'c_metric-BIC': 4}\n",
      "Creating Factor Tree\n",
      "No 4 factor solution computed yet! Computing...\n",
      "Determining Higher Order Factors\n",
      "# of components not specified, using BIC determined #\n",
      "Higher order factors could not be calculated\n",
      "*******************************************************************************\n",
      "Running HCA\n",
      "*******************************************************************************\n",
      "Clustering data\n",
      "Clustering EFA\n",
      "['Speeded IP', 'Strategic IP', 'Caution', 'Perc /Resp']\n",
      "Saving Subset: task\n",
      "*******************************************************************************\n",
      "SUBSET: SURVEY\n",
      "*******************************************************************************\n",
      "*******************************************************************************\n",
      "Analyzing Subset: survey\n",
      "Dataset imputed: number of variables, 61 , number of subjects, 386\n",
      "*******************************************************************************\n",
      "Running demographics\n",
      "*******************************************************************************\n",
      "Is the data adequate for factor analysis? Yes\n",
      "Determining Optimal Dimensionality\n",
      "Best Components:  {'c_metric-BIC': 6}\n",
      "Creating Factor Tree\n",
      "No 6 factor solution computed yet! Computing...\n",
      "Determining Higher Order Factors\n",
      "# of components not specified, using BIC determined #\n",
      "*******************************************************************************\n",
      "Running EFA, rotate: oblimin\n",
      "*******************************************************************************\n",
      "Is the data adequate for factor analysis? Yes\n",
      "Determining Optimal Dimensionality\n",
      "Best Components:  {'c_metric-BIC': 8}\n",
      "Creating Factor Tree\n",
      "No 8 factor solution computed yet! Computing...\n",
      "Determining Higher Order Factors\n",
      "# of components not specified, using BIC determined #\n",
      "*******************************************************************************\n",
      "Running HCA\n",
      "*******************************************************************************\n",
      "Clustering data\n",
      "Clustering EFA\n",
      "['Goal-directed/Mindfulness', 'Sensation Seeking', 'Emotional Control', 'Reward Sensitivity', 'Risk Perception', 'Ethical Risk Taking', 'Social Risk Taking', 'Agreeableness']\n",
      "Saving Subset: survey\n",
      "*******************************************************************************\n",
      "SUBSET: PSYCH\n",
      "*******************************************************************************\n",
      "*******************************************************************************\n",
      "Analyzing Subset: psych\n",
      "Dataset imputed: number of variables, 207 , number of subjects, 497\n",
      "*******************************************************************************\n",
      "Running demographics\n",
      "*******************************************************************************\n",
      "Is the data adequate for factor analysis? Yes\n",
      "Determining Optimal Dimensionality\n",
      "Best Components:  {'c_metric-BIC': 6}\n",
      "Creating Factor Tree\n",
      "No 6 factor solution computed yet! Computing...\n",
      "Determining Higher Order Factors\n",
      "# of components not specified, using BIC determined #\n",
      "*******************************************************************************\n",
      "Running EFA, rotate: oblimin\n",
      "*******************************************************************************\n",
      "Is the data adequate for factor analysis? Yes\n",
      "Determining Optimal Dimensionality\n",
      "Best Components:  {'c_metric-BIC': 7}\n",
      "Creating Factor Tree\n",
      "No 7 factor solution computed yet! Computing...\n",
      "Determining Higher Order Factors\n",
      "# of components not specified, using BIC determined #\n",
      "*******************************************************************************\n",
      "Running HCA\n",
      "*******************************************************************************\n",
      "Clustering data\n",
      "Clustering EFA\n",
      "3\n",
      "['Anxious-Depressed', 'Compulsive Intrusive Thoughts', 'Social Withdrawal']\n",
      "No 3 factor solution computed yet! Computing...\n",
      "Saving Subset: psych\n",
      "*******************************************************************************\n",
      "SUBSET: CRISIS\n",
      "*******************************************************************************\n",
      "*******************************************************************************\n",
      "Analyzing Subset: crisis\n",
      "Dataset imputed: number of variables, 37 , number of subjects, 2868\n",
      "*******************************************************************************\n",
      "Running demographics\n",
      "*******************************************************************************\n",
      "Is the data adequate for factor analysis? Yes\n",
      "Determining Optimal Dimensionality\n",
      "Best Components:  {'c_metric-BIC': 6}\n",
      "Creating Factor Tree\n",
      "No 6 factor solution computed yet! Computing...\n",
      "Determining Higher Order Factors\n",
      "# of components not specified, using BIC determined #\n",
      "*******************************************************************************\n",
      "Running EFA, rotate: oblimin\n",
      "*******************************************************************************\n",
      "Is the data adequate for factor analysis? Yes\n",
      "Determining Optimal Dimensionality\n",
      "Best Components:  {'c_metric-BIC': 10}\n",
      "Creating Factor Tree\n",
      "No 10 factor solution computed yet! Computing...\n",
      "Determining Higher Order Factors\n",
      "# of components not specified, using BIC determined #\n",
      "*******************************************************************************\n",
      "Running HCA\n",
      "*******************************************************************************\n",
      "Clustering data\n",
      "Clustering EFA\n",
      "['Negative Mood', 'COVID-19 Worries', 'Stress Life Changes', 'Sleep Time', 'Sleep Hours', 'Physical Exercise', 'Economic Concern', 'Changes Relationship', 'Media Usage', 'General Anxiety']\n",
      "Saving Subset: crisis\n"
     ]
    }
   ],
   "source": [
    "demographic_factor_names = ['Mental Health',\n",
    "                            'Drug Use',\n",
    "                            'Problem Drinking',\n",
    "                            'Daily Smoking',\n",
    "                            'Binge Drinking',\n",
    "                            'Lifetime Smoking']\n",
    "\n",
    "datafile = 'Covid_train'\n",
    "\n",
    "results = None\n",
    "all_results = None\n",
    "ID = str(random.getrandbits(16)) \n",
    "# create/run results for each subset\n",
    "for subset in subsets:\n",
    "    name = subset['name']\n",
    "    if selected_subsets is not None and name not in selected_subsets:\n",
    "        continue\n",
    "    if verbose:\n",
    "        print('*'*79)\n",
    "        print('SUBSET: %s' % name.upper())\n",
    "        print('*'*79)\n",
    "    if run_analysis == True:\n",
    "        print('*'*79)\n",
    "        print('Analyzing Subset: %s' % name)\n",
    "        # ****************************************************************************\n",
    "        # Filter Data and check number of variable selected is correct \n",
    "        # ****************************************************************************\n",
    "        print('Dataset imputed: number of variables,', len(data[name].columns),  ', number of subjects,', len(data[name]))\n",
    "        # ****************************************************************************\n",
    "        # Load Data\n",
    "        # ****************************************************************************\n",
    "        # run dimensional analysis\n",
    "        start = time.time()\n",
    "        results = Results(datafile=datafile,\n",
    "                          file = data[name],\n",
    "                          file_no_impute= data[name],\n",
    "                          file_demographics  = demo_data_training, \n",
    "                          dist_metric='abscorrelation',\n",
    "                          name=subset['name'],\n",
    "                          filter_regex='.',\n",
    "                          boot_iter=boot_iter,\n",
    "                          ID=ID,\n",
    "                          residualize_vars=['Age', 'Sex'])\n",
    "        results.run_demographic_analysis(verbose=verbose, bootstrap=bootstrap)\n",
    "        # ****************************************************************************\n",
    "        # EFA using optimal dimensionality\n",
    "        # ****************************************************************************\n",
    "        for rotate in ['oblimin']:\n",
    "            results.run_EFA_analysis(rotate=rotate, \n",
    "                                     verbose=verbose, \n",
    "                                     bootstrap=bootstrap)\n",
    "            results.run_clustering_analysis(rotate=rotate, \n",
    "                                            verbose=verbose, \n",
    "                                            run_graphs=False)\n",
    "            if subset['dimensionality'] is not None:\n",
    "                print(subset['dimensionality'])\n",
    "                results.EFA.results['num_factors'] = subset['dimensionality']\n",
    "            c = results.EFA.get_c()\n",
    "            # name factors\n",
    "            factor_names = subset.get('%s_factor_names' % rotate, None)\n",
    "            print(factor_names)\n",
    "            if factor_names:\n",
    "                results.EFA.name_factors(factor_names, rotate=rotate)\n",
    "        ID = results.ID.split('_')[1]\n",
    "        results.DA.name_factors(demographic_factor_names)\n",
    "        if verbose: print('Saving Subset: %s' % name)\n",
    "        id_file = results.save_results()\n",
    "        # ***************************** saving ****************************************\n",
    "        # copy latest results and prediction to higher directory\n",
    "        copyfile(id_file, path.join(path.dirname(results.get_output_dir()), \n",
    "                                    '%s_results.pkl' % name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all results from EFA on training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results  = load_results(datafile = 'Covid_train', verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DONT RUN THIS Check correlation between the original solution and the training solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = ['task', 'survey']\n",
    "sizes ={'task':30, 'survey':30}\n",
    "fonts= {'task':30, 'survey': 15}\n",
    "loadings_orig={}\n",
    "loadings_train={}\n",
    "corr_loadings ={}\n",
    "\n",
    "for subset in lists:\n",
    "    print('*'*79)\n",
    "    print(subset)\n",
    "    print('*'*79)\n",
    "    \n",
    "    # ****************************************************************************\n",
    "    # Obtain loadings for solution on 522 subjects [original]\n",
    "    # ****************************************************************************\n",
    "    loadings_orig[subset] = get_loadings('Complete_02-16-2019', subset = subset)\n",
    "    \n",
    "    # ****************************************************************************\n",
    "    # Obtain loadings for solution on 386 subjects [train]\n",
    "    # ****************************************************************************\n",
    "    loadings_train[subset] = get_loadings('Covid_train', subset = subset)\n",
    "   \n",
    "    # ****************************************************************************\n",
    "    # Remove suffixes added by the different preprocessing pipelines \n",
    "    # ****************************************************************************\n",
    "    remove_suffix_row(loadings_orig[subset] , '.logTr')\n",
    "    remove_suffix_row(loadings_orig[subset] , '.ReflogTr')\n",
    "    remove_suffix_row(loadings_train[subset], '.pt')\n",
    "    loadings_train[subset].index = [i.replace('_correctlayout', '') for i in loadings_train[subset].index]\n",
    "    \n",
    "    \n",
    "    # ****************************************************************************\n",
    "    # Keep only common variables \n",
    "    # ****************************************************************************\n",
    "    common_vars = set.intersection(set(loadings_orig[subset].index), \n",
    "                                   set(loadings_train[subset].index))\n",
    "    \n",
    "    \n",
    "    loadings_orig[subset] = drop_not_common_vars(loadings_orig[subset], common_vars = common_vars)\n",
    "    loadings_train[subset] = drop_not_common_vars(loadings_train[subset], common_vars = common_vars)\n",
    "    assert list(loadings_orig[subset].index) == list(loadings_train[subset].index), 'order rows does not match'\n",
    "    \n",
    "    # ****************************************************************************\n",
    "    # Add suffix to recognise data set\n",
    "    # ****************************************************************************\n",
    "    loadings_orig[subset]   = loadings_orig[subset].add_suffix(', Eisenberg et al., 2019')\n",
    "    loadings_train[subset]  = loadings_train[subset].add_suffix(', Train dataset')\n",
    "    \n",
    "    \n",
    "    # ****************************************************************************\n",
    "    # Concatenate data \n",
    "    # ****************************************************************************\n",
    "    temp_merged = pd.concat([loadings_orig[subset], loadings_train[subset]], axis=1)    \n",
    "    corr_loadings[subset] = temp_merged.corr(method='pearson')\n",
    "    \n",
    "    plot_dir = path.dirname(all_results[subset].get_plot_dir())\n",
    "    plot_correlation(corr_loadings[subset], size = sizes[subset],  \n",
    "                     subset = subset , plot_dir = plot_dir, fontsize = fonts[subset])\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "task\n",
      "Number of factors: 4\n",
      "EFA on N = 386 subjects\n",
      "Fit indexes:\n",
      "RMSEA index       RMSEA      lower      upper confidence \n",
      "0.06018724 0.05904872 0.06159523 0.90000000 \n",
      "\n",
      "BIC -20910.23644766232\n",
      "TLI 0.397542396282727\n",
      "rms, root mean square of the residuals (RMSA) 0.05963770151357489\n",
      "crms, df corrected root mean square of the residuals 0.061857394370673315\n",
      "*******************************************************************************\n",
      "*******************************************************************************\n",
      "survey\n",
      "Number of factors: 8\n",
      "EFA on N = 386 subjects\n",
      "Fit indexes:\n",
      "RMSEA index       RMSEA      lower      upper confidence \n",
      "0.06415604 0.06169961 0.06689240 0.90000000 \n",
      "\n",
      "BIC -4609.316891142318\n",
      "TLI 0.7815952127363427\n",
      "rms, root mean square of the residuals (RMSA) 0.0393012492478805\n",
      "crms, df corrected root mean square of the residuals 0.04542256097339117\n",
      "*******************************************************************************\n",
      "*******************************************************************************\n",
      "psych\n",
      "Number of factors: 3\n",
      "EFA on N = 497 subjects\n",
      "Fit indexes:\n",
      "RMSEA index       RMSEA      lower      upper confidence \n",
      "0.04448476 0.04394480 0.04520521 0.90000000 \n",
      "\n",
      "BIC -87430.08315538339\n",
      "TLI 0.545212877905949\n",
      "rms, root mean square of the residuals (RMSA) 0.05692118205172786\n",
      "crms, df corrected root mean square of the residuals 0.05776450475795749\n",
      "*******************************************************************************\n",
      "*******************************************************************************\n",
      "crisis\n",
      "Number of factors: 10\n",
      "EFA on N = 2868 subjects\n",
      "Fit indexes:\n",
      "RMSEA index       RMSEA      lower      upper confidence \n",
      "0.02881368 0.02700054 0.03065993 0.90000000 \n",
      "\n",
      "BIC -1561.7550599727058\n",
      "TLI 0.9593192806330962\n",
      "rms, root mean square of the residuals (RMSA) 0.014127001222947891\n",
      "crms, df corrected root mean square of the residuals 0.019742852816167583\n",
      "*******************************************************************************\n",
      "*******************************************************************************\n",
      "Demographics\n",
      "Number of factors: 6\n",
      "EFA on N = 386 subjects\n",
      "Fit indexes:\n",
      "RMSEA index       RMSEA      lower      upper confidence \n",
      "0.05285771 0.04952017 0.05645564 0.90000000 \n",
      "\n",
      "BIC -3312.9370616927135\n",
      "TLI 0.8293352573636811\n",
      "rms, root mean square of the residuals (RMSA) 0.04757005873722894\n",
      "crms, df corrected root mean square of the residuals 0.05463930670449176\n",
      "*******************************************************************************\n"
     ]
    }
   ],
   "source": [
    "EFA = {}\n",
    "\n",
    "for subset in selected_subsets:\n",
    "    print('*'*79)\n",
    "    print(subset)\n",
    "    EFA[subset]     = get_EFA(all_results[subset])\n",
    "    get_fit_indexes(EFA[subset])\n",
    "    \n",
    "print('*'*79)\n",
    "print(\"Demographics\")\n",
    "DA  = all_results['task'].DA\n",
    "get_fit_indexes(DA)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get cumulative variance explained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_task    =  EFA['task'].results['factor_tree_Rout_%s' % 'oblimin'][4]\n",
    "fa_survey  =  EFA['survey'].results['factor_tree_Rout_%s' % 'oblimin'][8]\n",
    "fa_psych   =  EFA['psych'].results['factor_tree_Rout_%s' % 'oblimin'][3]\n",
    "fa_crisis  =  EFA['crisis'].results['factor_tree_Rout_%s' % 'oblimin'][10]\n",
    "fa_demo    = DA.results['factor_tree_Rout_%s' % 'oblimin'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1] \"Task ***************************************************************\"\n",
       "                             ML1        ML3        ML2        ML4\n",
       "SS loadings           9.17150874 5.59693036 4.38650600 4.41732084\n",
       "Proportion Var        0.08116379 0.04953036 0.03881864 0.03909133\n",
       "Cumulative Var        0.08116379 0.13069415 0.16951279 0.20860412\n",
       "Proportion Explained  0.38908049 0.23743710 0.18608758 0.18739483\n",
       "Cumulative Proportion 0.38908049 0.62651758 0.81260517 1.00000000\n",
       "[1] \"Survey ***************************************************************\"\n",
       "                            ML1        ML2        ML4        ML8        ML3\n",
       "SS loadings           7.0276741 5.17681566 4.70950562 3.87847627 3.02238315\n",
       "Proportion Var        0.1152078 0.08486583 0.07720501 0.06358158 0.04954726\n",
       "Cumulative Var        0.1152078 0.20007360 0.27727861 0.34086019 0.39040746\n",
       "Proportion Explained  0.2261157 0.16656426 0.15152854 0.12479014 0.09724530\n",
       "Cumulative Proportion 0.2261157 0.39267997 0.54420851 0.66899864 0.76624395\n",
       "                             ML5        ML6        ML7\n",
       "SS loadings           2.90503605 2.22622793 2.13387192\n",
       "Proportion Var        0.04762354 0.03649554 0.03498151\n",
       "Cumulative Var        0.43803100 0.47452654 0.50950804\n",
       "Proportion Explained  0.09346966 0.07162898 0.06865742\n",
       "Cumulative Proportion 0.85971360 0.93134258 1.00000000\n",
       "[1] \"Psych ***************************************************************\"\n",
       "                             ML1         ML2         ML3\n",
       "SS loadings           23.8774769 17.80835879 14.78482227\n",
       "Proportion Var         0.1153501  0.08603072  0.07142426\n",
       "Cumulative Var         0.1153501  0.20138085  0.27280511\n",
       "Proportion Explained   0.4228298  0.31535596  0.26181424\n",
       "Cumulative Proportion  0.4228298  0.73818576  1.00000000\n",
       "[1] \"Crisis ***************************************************************\"\n",
       "                             ML2        ML5       ML7        ML1        ML3\n",
       "SS loadings           3.18871145 2.52609297 2.4313660 1.88505253 1.72456973\n",
       "Proportion Var        0.08618139 0.06827278 0.0657126 0.05094737 0.04660999\n",
       "Cumulative Var        0.08618139 0.15445417 0.2201668 0.27111414 0.31772413\n",
       "Proportion Explained  0.17844532 0.14136414 0.1360631 0.10549051 0.09650964\n",
       "Cumulative Proportion 0.17844532 0.31980946 0.4558725 0.56136305 0.65787269\n",
       "                             ML4        ML6        ML8        ML9       ML10\n",
       "SS loadings           1.56425261 1.52208295 0.87398380 0.95954395 1.19374772\n",
       "Proportion Var        0.04227710 0.04113738 0.02362118 0.02593362 0.03226345\n",
       "Cumulative Var        0.36000123 0.40113860 0.42475979 0.45069341 0.48295686\n",
       "Proportion Explained  0.08753804 0.08517816 0.04890951 0.05369759 0.06680400\n",
       "Cumulative Proportion 0.74541073 0.83058889 0.87949840 0.93319600 1.00000000\n",
       "[1] \"Demo ***************************************************************\"\n",
       "                             ML3        ML5       ML2        ML1        ML6\n",
       "SS loadings           4.21883424 3.36757499 3.4384799 2.60961719 2.37678427\n",
       "Proportion Var        0.08789238 0.07015781 0.0716350 0.05436702 0.04951634\n",
       "Cumulative Var        0.08789238 0.15805019 0.2296852 0.28405222 0.33356855\n",
       "Proportion Explained  0.23287364 0.18588534 0.1897992 0.14404715 0.13119511\n",
       "Cumulative Proportion 0.23287364 0.41875898 0.6085582 0.75260532 0.88380043\n",
       "                             ML4\n",
       "SS loadings           2.10511901\n",
       "Proportion Var        0.04385665\n",
       "Cumulative Var        0.37742520\n",
       "Proportion Explained  0.11619957\n",
       "Cumulative Proportion 1.00000000\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i fa_task -i fa_survey -i fa_psych  -i fa_crisis -i fa_demo\n",
    "print('Task ***************************************************************')\n",
    "print(fa_task$Vaccounted)\n",
    "print('Survey ***************************************************************')\n",
    "print(fa_survey$Vaccounted)\n",
    "print('Psych ***************************************************************')\n",
    "print(fa_psych$Vaccounted)\n",
    "print('Crisis ***************************************************************')\n",
    "print(fa_crisis$Vaccounted)\n",
    "print('Demo ***************************************************************')\n",
    "print(fa_demo$Vaccounted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot factor correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in selected_subsets:\n",
    "    c= EFA[subset].get_c()\n",
    "    plot_dir = path.dirname(all_results[subset].get_plot_dir())\n",
    "    plot_factor_correlation(all_results[subset],c, plot_dir = plot_dir, title=False)\n",
    "    \n",
    "c= DA.get_c()  \n",
    "plot_factor_correlation(all_results[subset],c, plot_dir = plot_dir, title=False, DA = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot summary of factor loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in selected_subsets:\n",
    "    c= EFA[subset].get_c()\n",
    "    plot_dir = path.dirname(all_results[subset].get_plot_dir())\n",
    "    plot_short_factor_loading(all_results[subset],c, subset, plot_dir = plot_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.pop('psych')        \n",
    "plot_BIC(all_results, size=20, ext='png', dpi=300, plot_dir = plot_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between DV and do cross prediction tasks and surveys from training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.pop('crisis')\n",
    "plot_dir = path.dirname(all_results['task'].get_plot_dir())\n",
    "\n",
    "\n",
    "survey_order = get_EFA_HCA(all_results['survey'], EFA= False)['reorder_vec']\n",
    "task_order   = get_EFA_HCA(all_results['task'], EFA= False)['reorder_vec']\n",
    "\n",
    "DVs_data = pd.concat([data['task'].iloc[:, task_order], data['survey'].iloc[:, survey_order]], axis =1)\n",
    "#Plot correlation map\n",
    "plot_corr_heatmap(DVs_data, survey_order, task_order, size=4.6*1/2, ext='png', dpi=300, plot_dir=plot_dir)\n",
    "#Run cross prediction\n",
    "run_cross_prediction(all_results, save = True)\n",
    "#Plot cross prediction\n",
    "plot_cross_within_prediction(path.join(path.dirname(all_results['task'].get_output_dir()), 'cross_prediction.pkl'), \n",
    "                                     size=4.6*1/2, ext='png', dpi=300, plot_dir=plot_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get testing datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "IMPORTING TESTING DATASET: task\n",
      "********************************************************************************\n",
      "Getting dataset: /SRO/Data_master/Complete_02-16-2019...:\n",
      "file: meaningful_variables_pt_imputed_test.csv \n",
      " \n",
      "Getting dataset: /SRO/Data/Complete_Covid_03-26-2021...:\n",
      "file: meaningful_variables_pt_imputed_test.csv \n",
      " \n",
      "Dropping ravens.score.pt as included as explanatory variable in stats model\n",
      "Pre COVID-19, # Subjs: 107 ; # DVs: 113\n",
      "Post COVID-19,  # Subjs: 107 ;  # DVs: 113 \n",
      "\n",
      "********************************************************************************\n",
      "IMPORTING TESTING DATASET: survey\n",
      "********************************************************************************\n",
      "Getting dataset: /SRO/Data_master/Complete_02-16-2019...:\n",
      "file: meaningful_variables_pt_imputed_test.csv \n",
      " \n",
      "Getting dataset: /SRO/Data/Complete_Covid_03-26-2021...:\n",
      "file: meaningful_variables_pt_imputed_test.csv \n",
      " \n",
      "Dropping ['bis11_survey.Attentional.pt', 'bis11_survey.Motor.pt', 'bis11_survey.Nonplanning.pt'] as BIS included for Psych\n",
      "Pre COVID-19, # Subjs: 107 ; # DVs: 61\n",
      "Post COVID-19,  # Subjs: 107 ;  # DVs: 61 \n",
      "\n",
      "********************************************************************************\n",
      "IMPORTING TESTING DATASET: psych\n",
      "********************************************************************************\n",
      "Getting dataset: /SRO/Data/Complete_Covid_03-26-2021...:\n",
      "file: subject_x_items_psych_sro.csv \n",
      " \n",
      "Dropping {'eat.9', 'eat.26'} because they had no variability in testing dataset\n",
      "Post COVID-19 # Subjs: 107 ;  # DVs: 207 \n",
      "\n",
      "********************************************************************************\n",
      "IMPORTING TESTING DATASET: crisis\n",
      "********************************************************************************\n",
      "Getting dataset: /SRO/Data/Complete_Covid_03-26-2021...:\n",
      "file: subject_x_items_crisis_all_surveys.csv \n",
      " \n",
      "Post COVID-19,  # Subjs: 107 ;  # DVs: 37 \n",
      "\n",
      "********************************************************************************\n",
      "IMPORTING TESTING DATASET: demo\n",
      "********************************************************************************\n",
      "Getting dataset: /SRO/Data_master/Complete_02-16-2019...:\n",
      "file: demographic_health.csv \n",
      " \n",
      "Pre COVID-19, # Subjs: 107 ; # DVs: 48\n",
      "Getting dataset: /SRO/Data/Complete_Covid_03-26-2021...:\n",
      "file: demographic_health.csv \n",
      " \n",
      "Post COVID-19, # Subjs: 107 ; # DVs: 48\n"
     ]
    }
   ],
   "source": [
    "master_test ={}\n",
    "covid_test ={}\n",
    "\n",
    "for subset in subsets:\n",
    "    name = subset['name']\n",
    "    if selected_subsets is not None and name not in selected_subsets:\n",
    "        continue\n",
    "    print('*'*80)\n",
    "    print('IMPORTING TESTING DATASET:', name)\n",
    "    print('*'*80)\n",
    "    if name =='psych':\n",
    "                \n",
    "        psych_sro = get_behav_data(file = 'subject_x_items_psych_sro.csv', verbose = True)\n",
    "        psych_sro, sro_colname_drop = drop_constant_column(psych_sro, 'SRO') \n",
    "        print('Dropping', sro_colname_drop, 'because they had no variability in testing dataset')\n",
    "        \n",
    "        covid_test[name] = psych_sro\n",
    "        print('Post COVID-19 # Subjs:', len(covid_test[name].index), ';  # DVs:' , len(covid_test[name].columns), '\\n')\n",
    "        \n",
    "    elif name == 'crisis':\n",
    "        crisis_sro = get_behav_data(file = 'subject_x_items_crisis_all_surveys.csv', verbose = True)\n",
    "        cols_crisis_training= data['crisis'].columns\n",
    "        crisis_sro = crisis_sro[cols_crisis_training]\n",
    "              \n",
    "        covid_test[name] = crisis_sro\n",
    "        print('Post COVID-19,  # Subjs:', len(covid_test[name].index), ';  # DVs:' , len(covid_test[name].columns), '\\n')\n",
    "              \n",
    "    else:\n",
    "        master_test[name] = get_behav_data(file = 'meaningful_variables_pt_imputed_test.csv', \n",
    "                                       data_subset = 'Data_master', \n",
    "                                       filter_regex =name, verbose=True)\n",
    "\n",
    "        covid_test[name] = get_behav_data(file = 'meaningful_variables_pt_imputed_test.csv', \n",
    "                                        data_subset = 'Data', \n",
    "                                        filter_regex =name, verbose=True)\n",
    "        \n",
    "\n",
    "        if name == 'task':\n",
    "            print('Dropping', list_vars_task, 'as included as explanatory variable in stats model')\n",
    "              \n",
    "            master_test[name] = master_test[name].drop(list_vars_task, axis=1)\n",
    "            covid_test[name]  = covid_test[name].drop(list_vars_task, axis=1)\n",
    "\n",
    "            \n",
    "        if name =='survey': \n",
    "\n",
    "            print('Dropping', list_vars_survey ,'as BIS included for Psych')\n",
    "              \n",
    "            master_test[name] = master_test[name].drop(list_vars_survey, axis=1)\n",
    "            covid_test[name]  = covid_test[name].drop(list_vars_survey, axis=1)\n",
    "              \n",
    "        print('Pre COVID-19, # Subjs:', len(master_test[name].index), '; # DVs:', len(master_test[name].columns))\n",
    "        print('Post COVID-19,  # Subjs:', len(covid_test[name].index), ';  # DVs:' , len(covid_test[name].columns), '\\n')\n",
    "        \n",
    "print('*'*80)\n",
    "print('IMPORTING TESTING DATASET:', 'demo')\n",
    "print('*'*80)\n",
    "\n",
    "\n",
    "#For Health Outcome Measures a bit more work is needed \n",
    "loadings = DA.get_loading(DA.get_c(), rotate='oblimin')\n",
    "list_vars_DA = list(loadings.index)\n",
    "list_vars_DA.append('Age')\n",
    "list_vars_DA.append('Sex')\n",
    "\n",
    "\n",
    "master_test['demo'] = get_behav_data(file = 'demographic_health.csv', data_subset = 'Data_master', verbose=True)\n",
    "master_test['demo'] = master_test['demo'].loc[master_test['task'].index] #Pick only test subjects \n",
    "master_test['demo'] = master_test['demo'][master_test['demo'].columns[master_test['demo'].columns.isin(list_vars_DA)]]\n",
    "master_test['demo']= residualize_baseline(master_test['demo'] , ['Age', 'Sex']) #Residualize age and gender\n",
    "print('Pre COVID-19, # Subjs:', len(master_test['demo'].index), '; # DVs:', len(master_test['demo'].columns))\n",
    "\n",
    "\n",
    "#=========================================================\n",
    "#Post COVID-9\n",
    "#=========================================================\n",
    "covid_test['demo'] = get_behav_data(file = 'demographic_health.csv', data_subset = 'Data', verbose=True)\n",
    "\n",
    "#*********************************************************\n",
    "# Quick pre-processing as unit of measures for \n",
    "# Longest relationship and Height were different\n",
    "# for pre and post covid data collection\n",
    "#*********************************************************\n",
    "#Longest relationship in months \n",
    "covid_test['demo']['LongestRelationship'] = (covid_test['demo']['LongestRelationshipYears']*12) + covid_test['demo']['LongestRelationshipMonths']\n",
    "#Heigth in hinches\n",
    "covid_test['demo']['HeightInches'] = (covid_test['demo']['HeightFeet']*12) +  covid_test['demo']['HeightInches']\n",
    "#Outcome measures are pre and post, keep only post - pre is given by master \n",
    "covid_test['demo']=covid_test['demo'].loc[:,~covid_test['demo'].columns.str.contains('_pre')]\n",
    "#Replace post with empty string to match variables name from eisenberg\n",
    "covid_test['demo'].columns=covid_test['demo'].columns.str.replace(\"_post\", \"\")\n",
    "#*********************************************************\n",
    "covid_test['demo'] = covid_test['demo'][covid_test['demo'].columns[covid_test['demo'].columns.isin(list_vars_DA)]]\n",
    "covid_test['demo']  = residualize_baseline(covid_test['demo'] , ['Age', 'Sex'])\n",
    "print('Post COVID-19, # Subjs:', len(covid_test['demo'].index), '; # DVs:', len(covid_test['demo'].columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict subject's scores from factor solution on training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=path.join(get_info('base_directory'),'Results/dimensional_structure/Covid_test')\n",
    "factor_names = {}\n",
    "for subset in subsets:\n",
    "    name = subset['name']\n",
    "    factor_names[name] = subset.get('%s_factor_names' % 'oblimin', None)\n",
    "\n",
    "factor_names_task   = factor_names['task']\n",
    "factor_names_survey = factor_names['survey']\n",
    "factor_names_crisis = factor_names['crisis']\n",
    "factor_names_psych  = factor_names['psych']\n",
    "\n",
    "\n",
    "master_test_task   = master_test['task']\n",
    "master_test_survey = master_test['survey']\n",
    "master_test_demo   =  master_test['demo']\n",
    "\n",
    "covid_test_task    = covid_test['task']\n",
    "covid_test_survey  = covid_test['survey']\n",
    "covid_test_demo    =  covid_test['demo']\n",
    "\n",
    "covid_test_psych   = covid_test['psych']\n",
    "covid_test_crisis  = covid_test['crisis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i fa_task -i factor_names_task -i master_test_task -i covid_test_task -i output_dir\n",
    "\n",
    "master_test_scores <-  as.data.frame(predict(fa_task, master_test_task))\n",
    "covid_test_scores  <-  as.data.frame(predict(fa_task, covid_test_task))\n",
    "\n",
    "colnames(master_test_scores) <- c(factor_names_task)\n",
    "colnames(covid_test_scores) <-  c(factor_names_task)\n",
    "\n",
    "write.csv(master_test_scores, file.path(output_dir, \"master_test_scores_task.csv\"), row.names=TRUE)\n",
    "write.csv(covid_test_scores, file.path(output_dir,  \"covid_test_scores_task.csv\"), row.names=TRUE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i fa_survey -i factor_names_survey -i master_test_survey -i covid_test_survey -i output_dir\n",
    "\n",
    "master_test_scores <-  as.data.frame(predict(fa_survey, master_test_survey))\n",
    "covid_test_scores  <-  as.data.frame(predict(fa_survey, covid_test_survey))\n",
    "\n",
    "colnames(master_test_scores) <-c(factor_names_survey)\n",
    "colnames(covid_test_scores)  <-c(factor_names_survey)\n",
    "\n",
    "\n",
    "write.csv(master_test_scores, file.path(output_dir, \"master_test_scores_survey.csv\"), row.names=TRUE)\n",
    "write.csv(covid_test_scores,  file.path(output_dir,  \"covid_test_scores_survey.csv\"), row.names=TRUE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Psych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i fa_psych -i factor_names_psych -i covid_test_psych -i output_dir\n",
    "\n",
    "covid_test_scores  <-  as.data.frame(predict(fa_psych, covid_test_psych))\n",
    "colnames(covid_test_scores)  <-c(factor_names_psych)\n",
    "\n",
    "\n",
    "write.csv(covid_test_scores,  file.path(output_dir,  \"covid_test_scores_psych.csv\"), row.names=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i fa_demo -i demographic_factor_names  -i master_test_demo -i covid_test_demo -i output_dir\n",
    "\n",
    "master_test_scores <-  as.data.frame(predict(fa_demo, master_test_demo))\n",
    "covid_test_scores  <-  as.data.frame(predict(fa_demo, covid_test_demo))\n",
    "\n",
    "colnames(master_test_scores) <- c(demographic_factor_names)\n",
    "colnames(covid_test_scores) <-  c(demographic_factor_names)\n",
    "\n",
    "\n",
    "write.csv(master_test_scores, file.path(output_dir, \"master_test_scores_demo.csv\"), row.names=TRUE)\n",
    "write.csv(covid_test_scores, file.path(output_dir,  \"covid_test_scores_demo.csv\"), row.names=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i fa_crisis -i factor_names_crisis -i covid_test_crisis -i output_dir\n",
    "\n",
    "covid_test_scores  <-  as.data.frame(predict(fa_crisis, covid_test_crisis))\n",
    "colnames(covid_test_scores)  <-c(factor_names_crisis)\n",
    "\n",
    "\n",
    "write.csv(covid_test_scores,  file.path(output_dir,  \"covid_test_scores_crisis.csv\"), row.names=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get final dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At master and covid assessment the same turker was assigned different subjectID . \n",
    "#This file stores the conversion. How this file was obtained is in admin folder, storing turker's ID hence not shared\n",
    "basedir=get_info('base_directory')\n",
    "name_master_covid = get_admin_data(path.join(basedir, 'dimensional_structure'), 'sro_name_master_covid.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_import_regex  = {'demo'    : [None , None ], \n",
    "                      'iq'     :  ['ravens.score.pt','ravens.score.pt'], \n",
    "                      'stress' :  ['stress_pre', 'stress_post'], \n",
    "                      'mindset':  ['mindset', 'mindset']}\n",
    "\n",
    "dict_import_folder = {'demo'    : ['Data_master' , 'Data'], \n",
    "                      'iq'      : ['Data_master' , 'Data'], \n",
    "                      'stress'  : ['Data' , 'Data'], #pre and post measures of stress collected only at Data timepoint\n",
    "                      'mindset' : ['Data' , 'Data']} #pre and post measures of mindset collected only at Data timepoint\n",
    "\n",
    "dict_import_file   = {'demo'    : ['demographics.csv', 'demographics.csv'], \n",
    "                      'iq'      : ['meaningful_variables_pt_imputed_test.csv', 'meaningful_variables_pt_imputed_test.csv'], \n",
    "                      'stress'  : ['meaningful_variables_stress_mindset_pt_imputed.csv', 'meaningful_variables_stress_mindset_pt_imputed.csv'], \n",
    "                      'mindset' : ['meaningful_variables_stress_mindset_pt_imputed.csv', 'meaningful_variables_stress_mindset_pt_imputed.csv'], \n",
    "                      'task'    : ['master_test_scores_task.csv', 'covid_test_scores_task.csv'], \n",
    "                      'survey'  : ['master_test_scores_survey.csv', 'covid_test_scores_survey.csv'], \n",
    "                      'psych'   : ['covid_test_scores_psych.csv', 'covid_test_scores_psych.csv'], #predicted scores for psych only at Data timepoint\n",
    "                      'crisis'  : ['covid_test_scores_crisis.csv', 'covid_test_scores_crisis.csv']}#predicted scores for crisis only at Data timepoint\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Data_master\n",
      "********************************************************************************\n",
      "iq\n",
      "Getting dataset: /SRO/Data_master/Complete_02-16-2019...:\n",
      "file: meaningful_variables_pt_imputed_test.csv \n",
      " \n",
      "task\n",
      "Getting dataset: /SRO/Results/dimensional_structure/Covid_test/master_test_scores_task.csv\n",
      "survey\n",
      "Getting dataset: /SRO/Results/dimensional_structure/Covid_test/master_test_scores_survey.csv\n",
      "crisis\n",
      "Getting dataset: /SRO/Results/dimensional_structure/Covid_test/covid_test_scores_crisis.csv\n",
      "mindset\n",
      "Getting dataset: /SRO/Data/Complete_Covid_03-26-2021...:\n",
      "file: meaningful_variables_stress_mindset_pt_imputed.csv \n",
      " \n",
      "stress\n",
      "Getting dataset: /SRO/Data/Complete_Covid_03-26-2021...:\n",
      "file: meaningful_variables_stress_mindset_pt_imputed.csv \n",
      " \n",
      "psych\n",
      "Getting dataset: /SRO/Results/dimensional_structure/Covid_test/covid_test_scores_psych.csv\n",
      "demo\n",
      "Getting dataset: /SRO/Data_master/Complete_02-16-2019...:\n",
      "file: demographics.csv \n",
      " \n",
      "********************************************************************************\n",
      "Data\n",
      "********************************************************************************\n",
      "iq\n",
      "Getting dataset: /SRO/Data/Complete_Covid_03-26-2021...:\n",
      "file: meaningful_variables_pt_imputed_test.csv \n",
      " \n",
      "task\n",
      "Getting dataset: /SRO/Results/dimensional_structure/Covid_test/covid_test_scores_task.csv\n",
      "survey\n",
      "Getting dataset: /SRO/Results/dimensional_structure/Covid_test/covid_test_scores_survey.csv\n",
      "crisis\n",
      "Getting dataset: /SRO/Results/dimensional_structure/Covid_test/covid_test_scores_crisis.csv\n",
      "mindset\n",
      "Getting dataset: /SRO/Data/Complete_Covid_03-26-2021...:\n",
      "file: meaningful_variables_stress_mindset_pt_imputed.csv \n",
      " \n",
      "stress\n",
      "Getting dataset: /SRO/Data/Complete_Covid_03-26-2021...:\n",
      "file: meaningful_variables_stress_mindset_pt_imputed.csv \n",
      " \n",
      "psych\n",
      "Getting dataset: /SRO/Results/dimensional_structure/Covid_test/covid_test_scores_psych.csv\n",
      "demo\n",
      "Getting dataset: /SRO/Data/Complete_Covid_03-26-2021...:\n",
      "file: demographics.csv \n",
      " \n"
     ]
    }
   ],
   "source": [
    "final_master ={}\n",
    "final_covid={}\n",
    "times = ['Data_master', 'Data']\n",
    "\n",
    "for i, time in enumerate(times): \n",
    "    print('*' *80)\n",
    "    print(time)\n",
    "    print('*' *80)\n",
    "    for name in dict_import_file.keys():\n",
    "        print(name)\n",
    "        if name in ['demo', 'iq', 'stress', 'mindset']:  # Set of raw variables, else import predicted scores\n",
    "            \n",
    "            if time =='Data_master': \n",
    "                final_master[name] = get_behav_data(file         = dict_import_file[name][i], \n",
    "                                                    data_subset  = dict_import_folder[name][i], \n",
    "                                                    filter_regex = dict_import_regex[name][i], \n",
    "                                                    verbose=True)\n",
    "            elif time == 'Data': \n",
    "                final_covid[name] = get_behav_data(file          = dict_import_file[name][i], \n",
    "                                                    data_subset  = dict_import_folder[name][i], \n",
    "                                                    filter_regex = dict_import_regex[name][i], \n",
    "                                                    verbose=True)\n",
    "        else: \n",
    "            if time =='Data_master': \n",
    "                final_master[name] = get_predicted_score_data(output_dir,dict_import_file[name][i], verbose = True)\n",
    "                \n",
    "            elif time == 'Data': \n",
    "                final_covid[name]  = get_predicted_score_data(output_dir,dict_import_file[name][i], verbose = True)\n",
    "\n",
    "#Remove labels related to time to concatenate later                \n",
    "final_master['stress'].columns = [col.replace('_pre.pt', '') for col in final_master['stress']]\n",
    "final_covid['stress'].columns = [col.replace('_post.pt', '') for col in final_covid['stress']]\n",
    "\n",
    "#Keep only testing subjects, and rename people so that they have the same id as the one given at covid test                \n",
    "final_master['demo']   = final_master['demo'].loc[list(final_master['task'].index), :]\n",
    "final_master['demo']   = final_master['demo'].rename(index=name_master_covid)\n",
    "final_master['iq']     = final_master['iq'].rename(index=name_master_covid)\n",
    "final_master['task']   = final_master['task'].rename(index=name_master_covid)\n",
    "final_master['survey'] = final_master['survey'].rename(index=name_master_covid)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove subjects with declared information not trustworthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of subjects declaring an age which does not match time elapsed between assessments: 8\n",
      "['s013', 's021', 's036', 's044', 's070', 's075', 's081', 's115']\n",
      "Drop these subjects as incosistent on at least one of the other variables:\n",
      " ['s013', 's021', 's036', 's070', 's081']\n"
     ]
    }
   ],
   "source": [
    "subjects_inconsistent_age = check_age(final_master['demo']['Age'], final_covid['demo']['Age'], 0, 4)\n",
    "final_covid = impute_age(subjects_inconsistent_age, final_covid, final_master, 4)\n",
    "vars_qc = ['HispanicLatino','HighestEducation','DivorceCount', 'RelationshipNumber','TrafficAccidentsLifeCount', 'ArrestedChargedLifeCount']\n",
    "drop_subjects  = check_demo_vars(vars_qc, subjects_inconsistent_age,final_master, final_covid )\n",
    "\n",
    "for df in final_master.keys():\n",
    "    final_master[df].drop(drop_subjects, inplace =True)\n",
    "    \n",
    "for df in final_covid.keys():\n",
    "    final_covid[df].drop(drop_subjects, inplace =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "#Concatenate files\n",
    "master_df = pd.concat([final_master['task'],\n",
    "                       final_master['survey'],\n",
    "                       final_master['psych'], \n",
    "                       final_master['mindset'], \n",
    "                       final_master['crisis'], \n",
    "                       final_master['stress'], \n",
    "                       final_master['demo'][['Age', 'Sex']], \n",
    "                       final_master['iq']], \n",
    "                       axis = 1)\n",
    "master_df['time'] = 'master'\n",
    "\n",
    "covid_df = pd.concat([final_covid['task'],\n",
    "                       final_covid['survey'],\n",
    "                       final_covid['psych'], \n",
    "                       final_covid['mindset'], \n",
    "                       final_covid['crisis'], \n",
    "                       final_covid['stress'], \n",
    "                       final_covid['demo'][['Age', 'Sex']], \n",
    "                       final_covid['iq']], \n",
    "                       axis = 1)\n",
    "covid_df['time'] = 'covid'\n",
    "all_df = pd.concat([master_df, covid_df], axis = 0)\n",
    "\n",
    "#Make name of variables easier to handle\n",
    "all_df['sid'] = all_df.index\n",
    "all_df = get_short_names(all_df)\n",
    "\n",
    "#Remove the 5 subjects to drop from both time points\n",
    "print(len(all_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get paired difference for variable acquired longitudinally \n",
    "data_all_paired_diff = pd.DataFrame([])\n",
    "list_of_variables = ['SpeededIP', 'StrategicIP', 'Caution', 'PercResp', 'lon', 'pss', 'soc_supp',\n",
    "                     'GDM', 'SS', 'EMC', 'RS', 'RP',  'ERT', 'SRT', 'AGR']\n",
    "\n",
    "for var in list_of_variables: \n",
    "    df_var_of_interest           = get_var_of_interest(all_df, var)\n",
    "    paired_diff                  = get_paired_diff(df_var_of_interest, var)    \n",
    "    data_all_paired_diff         = pd.concat([data_all_paired_diff, paired_diff], ignore_index = False, axis = 1)\n",
    "\n",
    "#Add variables used as covariates in the model for consistent visualization\n",
    "covariates = all_df[['Age', 'Sex', 'ravens.score.pt', 'AD', 'CIT', 'SW', 'mindset_stress.pt',  'mindset_pandemic_catastrophe.pt']][all_df['time'] == 'master']\n",
    "data_all_paired_diff= pd.concat([data_all_paired_diff, covariates], ignore_index = False, axis = 1) \n",
    "\n",
    "#Save\n",
    "data_all_paired_diff.to_csv(path.join(output_dir , 'data_all_paired_diff_final.csv'))\n",
    "all_df.to_csv(path.join(output_dir , 'data_all_df_final.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for prediction analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing age, gender, and IQ from: psych_DVs\n",
      "# Subjs: 102 ;    # DVs: 9 \n",
      "\n",
      "Removing age, gender, and IQ from: psych_fac\n",
      "# Subjs: 102 ;    # DVs: 3 \n",
      "\n",
      "Subset of data: task\n",
      "Getting dataset: /SRO/Data_master/Complete_02-16-2019...:\n",
      "file: meaningful_variables_pt_imputed_test.csv \n",
      " \n",
      "Subset of data: survey\n",
      "Getting dataset: /SRO/Data_master/Complete_02-16-2019...:\n",
      "file: meaningful_variables_pt_imputed_test.csv \n",
      " \n",
      "Subset of data: task\n",
      "Getting dataset: /SRO/Data/Complete_Covid_03-26-2021...:\n",
      "file: meaningful_variables_pt_imputed_test.csv \n",
      " \n",
      "Subset of data: survey\n",
      "Getting dataset: /SRO/Data/Complete_Covid_03-26-2021...:\n",
      "file: meaningful_variables_pt_imputed_test.csv \n",
      " \n",
      "task_covid\n",
      "task_master\n",
      "survey_covid\n",
      "survey_master\n",
      "***\n",
      "task_covid\n",
      "task_master\n",
      "survey_covid\n",
      "survey_master\n"
     ]
    }
   ],
   "source": [
    "raw_data ={}\n",
    "clustering = {}\n",
    "target_scores ={}\n",
    "factor_scores = {}\n",
    "\n",
    "#======================================================================================================\n",
    "# Get psych variables, either DVs or factor scores wich are the target scores and remover gender, age and IQ\n",
    "#======================================================================================================\n",
    "target_scores['psych_DVs'] = get_behav_data(file = 'meaningful_variables_psych_pt_imputed.csv', \n",
    "                                        data_subset = 'Data', filter_regex = '.', verbose= False)\n",
    "target_scores['psych_DVs'].drop(drop_subjects , inplace = True)\n",
    "\n",
    "target_scores['psych_fac'] = final_covid['psych']\n",
    "\n",
    "#Regress out gender, age, and IQ\n",
    "raven = final_covid['iq']\n",
    "age   = final_covid['demo']['Age']\n",
    "sex   = final_covid['demo']['Sex']\n",
    "for data in target_scores.copy(): \n",
    "    print('Removing age, gender, and IQ from:',  data)\n",
    "    df = pd.concat([sex,age , raven, target_scores[data]],  axis =1)\n",
    "    new_name= data + '_regressed'\n",
    "    target_scores[new_name]    = residualize_baseline(df, ['Age', 'Sex', 'ravens.score.pt'])\n",
    "    print(  '# Subjs:', len(target_scores[data].index), ';    # DVs:' , \n",
    "                        len(target_scores[data].columns), '\\n')  \n",
    "    \n",
    "#======================================================================================================\n",
    "# Get raw data and factor scores for tasks and surveys\n",
    "#======================================================================================================\n",
    "dictionary  = {\"Data_master\": \"master\", \"Data\": \"covid\"}\n",
    "directories = ['Data_master', 'Data']\n",
    "names = ['task', 'survey']\n",
    "\n",
    "for i, directory in enumerate(directories):     \n",
    "    for i, name in enumerate(names):         \n",
    "        name_dataset= name+'_'+ dictionary.get(directory)\n",
    "        print('Subset of data:', name)\n",
    "        \n",
    "        raw_data[name_dataset] = get_behav_data(file = 'meaningful_variables_pt_imputed_test.csv', \n",
    "                                        data_subset = directory, filter_regex =name, verbose=True)\n",
    "        #======================================================================\n",
    "        #If data master , need to rename subjects with same labels they get for target\n",
    "        #======================================================================  \n",
    "        if directory =='Data_master': \n",
    "            raw_data[name_dataset] = raw_data[name_dataset].rename(index = name_master_covid)  \n",
    "            \n",
    "        #======================================================================\n",
    "        #Drop subjects who do are not trustworthy\n",
    "        #======================================================================  \n",
    "        raw_data[name_dataset].drop(drop_subjects, inplace = True)\n",
    "            \n",
    "        #======================================================================\n",
    "        #Drop DVs not used in EFA for tasks/surveys\n",
    "        #======================================================================  \n",
    "        if name =='task': \n",
    "            raw_data[name_dataset]    = raw_data[name_dataset].drop(list_vars_task, axis=1)\n",
    "        \n",
    "        if name == 'survey': \n",
    "            raw_data[name_dataset]   = raw_data[name_dataset].drop(list_vars_survey, axis=1)\n",
    "\n",
    "        #======================================================================\n",
    "        #Get Clustering\n",
    "        #======================================================================      \n",
    "        results = load_results(datafile = 'Covid_train', name = name, verbose = False)\n",
    "        EFA = get_EFA(results[name])\n",
    "        c   = EFA.get_c()\n",
    "        HCA = get_HCA(results[name])\n",
    "        clustering[name_dataset]=HCA.results['EFA%s_%s' % (c, 'oblimin')]\n",
    "        \n",
    "        raw_data[name_dataset] = raw_data[name_dataset].sort_index()\n",
    "\n",
    "#Factor scores \n",
    "factor_scores['task_master']   = final_master['task'].sort_index()\n",
    "factor_scores['survey_master'] = final_master['survey'].sort_index()\n",
    "factor_scores['task_covid']    = final_covid['task'].sort_index()\n",
    "factor_scores['survey_covid']  = final_covid['survey'].sort_index()\n",
    "\n",
    "\n",
    "#Make sure that target and features matrixes are in the same order \n",
    "for data in factor_scores.keys(): \n",
    "    print(data)\n",
    "    assert np.array_equal(target_scores['psych_fac_regressed'].index , factor_scores[data].index),'dfs rows not in the same order'\n",
    "    assert np.array_equal(target_scores['psych_DVs_regressed'].index , factor_scores[data].index), 'dfs rows not in the same order'\n",
    "print('***')\n",
    "for data in raw_data.keys(): \n",
    "    print(data)\n",
    "    assert np.array_equal(target_scores['psych_fac_regressed'].index , raw_data[data].index),'dfs rows not in the same order'\n",
    "    assert np.array_equal(target_scores['psych_DVs_regressed'].index , raw_data[data].index), 'dfs rows not in the same order'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run prediction analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=path.join(get_info('base_directory'),'Results/dimensional_structure/Covid_test')\n",
    "directories = ['Data_master', 'Data']\n",
    "classifiers = [ 'ridge']\n",
    "subsets = ['task', 'survey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, directory in enumerate(directories):     \n",
    "    for subset in subsets:\n",
    "        name = subset\n",
    "        name_dataset= name+'_'+ dictionary.get(directory)\n",
    "        print('*'*79)\n",
    "        print('Running prediction: %s' % name_dataset)\n",
    "        for classifier in classifiers:\n",
    "            print('Running Classifier: %s' % classifier)\n",
    "            for rotate in ['oblimin']:\n",
    "                #PYSCH FACTOR SCORES AS TARGET\n",
    "                #No shuffle\n",
    "                run_prediction_local(factor_scores = factor_scores[name_dataset], \n",
    "                target_scores = target_scores['psych_fac_regressed'], \n",
    "                raw_data      = raw_data[name_dataset], \n",
    "                clustering    = clustering[name_dataset],\n",
    "                name          = name_dataset,                     \n",
    "                output_dir    = output_dir, \n",
    "                classifier    = classifier, rotate=rotate, shuffle=False,  verbose=True)\n",
    "            \n",
    "                #Shuffle\n",
    "                run_prediction_local(factor_scores = factor_scores[name_dataset], \n",
    "                target_scores = target_scores['psych_fac_regressed'],\n",
    "                raw_data      = raw_data[name_dataset], \n",
    "                clustering    = clustering[name_dataset],\n",
    "                name          = name_dataset,\n",
    "                output_dir    = output_dir, \n",
    "                classifier    = classifier, rotate=rotate, shuffle=2,  verbose=True) #2500\n",
    "            \n",
    "            \n",
    "                #PYSCH DVs AS TARGET\n",
    "                #No shuffle\n",
    "                run_prediction_local(factor_scores = factor_scores[name_dataset], \n",
    "                target_scores = target_scores['psych_DVs_regressed'],\n",
    "                raw_data      = raw_data[name_dataset], \n",
    "                clustering    = clustering[name_dataset],\n",
    "                name          = name_dataset,                     \n",
    "                output_dir    = output_dir, \n",
    "                classifier    = classifier, predicting_DVs = True, rotate=rotate, shuffle=False,  verbose=True)\n",
    "                \n",
    "                #Shuffle\n",
    "                run_prediction_local(factor_scores = factor_scores[name_dataset], \n",
    "                target_scores = target_scores['psych_DVs_regressed'],\n",
    "                raw_data      = raw_data[name_dataset], \n",
    "                clustering    = clustering[name_dataset],\n",
    "                name          = name_dataset,                     \n",
    "                output_dir    = output_dir, \n",
    "                classifier    = classifier, predicting_DVs = True, rotate=rotate, shuffle=1,  verbose=True) #2500\n",
    "                \n",
    "        # ***************************** saving ****************************************\n",
    "        prediction_dir = path.join(output_dir, 'prediction_outputs' , name_dataset)\n",
    "        new_dir = path.join(output_dir, 'prediction_outputs/', directory)\n",
    "        \n",
    "        if not path.exists(new_dir):\n",
    "            makedirs(new_dir)\n",
    "        for classifier in classifiers:\n",
    "            for change_flag in [False, True]:\n",
    "                print('Change flag: %s' % change_flag)\n",
    "                for subset in ['oblimin', 'raw']:\n",
    "                    prediction_files = glob(path.join(prediction_dir, '*%s*' % classifier))\n",
    "                    #Filter by change\n",
    "                    prediction_files = filter(lambda x: ('change' in x) == change_flag, prediction_files)\n",
    "                    #Filter by rotate\n",
    "                    prediction_files = filter(lambda x: subset in x, prediction_files)\n",
    "                    #Sort by creation file \n",
    "                    prediction_files = sorted(prediction_files, key = path.getmtime)[-4:]\n",
    "\n",
    "                    for filey in prediction_files:\n",
    "                        filename = '_'.join(path.basename(filey).split('_')[:-1])\n",
    "                        copyfile(filey, path.join(new_dir, '%s_%s.pkl' % (name, filename)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictions bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in subsets: \n",
    "    classifier = 'ridge'\n",
    "    outcome    = 'factors'\n",
    "    ext =      'pdf'\n",
    "    filename = 'EFA_%s_%s_%s_prediction_bar.%s' % (classifier, outcome, name, ext)\n",
    "    plot_dir = path.join(output_dir, 'prediction_outputs/ Plots/', filename)\n",
    "    predictions, insample_predictions, shuffled, insample_shuffled= get_predictions(output_dir, name , outcome)\n",
    "    plot_prediction(predictions, insample_predictions, shuffled, insample_shuffled, set_vars = name, \n",
    "                    comparison_label = '95% shuffled prediction', size = 10, filename=plot_dir, \n",
    "                    metric ='R2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=path.join(get_info('base_directory'),'Results/dimensional_structure/Covid_test')\n",
    "directories = ['Data_master', 'Data']\n",
    "classifiers = [ 'ridge']\n",
    "subsets = ['task', 'survey']\n",
    "\n",
    "\n",
    "for name in subsets: \n",
    "    for directory in directories:\n",
    "        classifier = 'ridge'\n",
    "        outcome    = 'factors'\n",
    "        ext = 'pdf'\n",
    "    \n",
    "        filename = 'EFA_%s_%s_%s_fingerprint.%s' % (classifier, outcome, name, ext)\n",
    "        plot_dir = path.join(output_dir, 'prediction_outputs/ Plots/', directory, filename)\n",
    "        plot_fingerprint(output_dir, directory, outcome, name, filename=plot_dir)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
